{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSXi70r+Mv48W8ZJmcSYTz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camsanjose/Thesis_CSJ/blob/main/Thesis_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMQqAg7i6mpL",
        "outputId": "d553bbeb-465f-41c6-b649-0f7f048de1cf"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P58VrK8vQIE"
      },
      "source": [
        "# Installations\r\n",
        "import sys\r\n",
        "if 'google.colab' in sys.modules:\r\n",
        "    !pip install emoji --upgrade\r\n",
        "    !pip install pandas-profiling==2.*\r\n",
        "    !pip install plotly==4.*\r\n",
        "    !python -m spacy download en_core_web_lg\r\n",
        "    !pip install pyldavis\r\n",
        "    !pip install gensim\r\n",
        "    !pip install chart_studio\r\n",
        "    !pip install --upgrade autopep8\r\n",
        "    !pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad9oN4TuvOGV"
      },
      "source": [
        "import sys\r\n",
        "if 'google.colab' in sys.modules:\r\n",
        "  !python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdRScWwN6_w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5c89f0-9e74-4dd3-d0b0-adf775ebeb13"
      },
      "source": [
        "# Required Libraries\r\n",
        "\r\n",
        "#Base and Cleaning \r\n",
        "import json\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import emoji\r\n",
        "import regex\r\n",
        "import re\r\n",
        "import string\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "#Visualizations\r\n",
        "import plotly.express as px\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import pyLDAvis.gensim\r\n",
        "import chart_studio\r\n",
        "import chart_studio.plotly as py \r\n",
        "import chart_studio.tools as tls\r\n",
        "\r\n",
        "#Natural Language Processing (NLP)\r\n",
        "import spacy\r\n",
        "import gensim\r\n",
        "from spacy.tokenizer import Tokenizer\r\n",
        "from gensim.corpora import Dictionary\r\n",
        "from gensim.models.ldamulticore import LdaMulticore\r\n",
        "from gensim import models\r\n",
        "from gensim.models.coherencemodel import CoherenceModel\r\n",
        "from gensim.parsing.preprocessing import STOPWORDS as SW\r\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from pprint import pprint\r\n",
        "from wordcloud import STOPWORDS\r\n",
        "stopwords = set(STOPWORDS)\r\n",
        "import nltk \r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning:\n",
            "\n",
            "`scipy.sparse.sparsetools` is deprecated!\n",
            "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEpK_dppwXNs"
      },
      "source": [
        "# Load spacy for LEMMATIZATION##################################################\r\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvFYhlAY7CWg"
      },
      "source": [
        "#TFIDF#####################################################################################################################3\r\n",
        "#download the un-pooled dataset \r\n",
        "df=pd.read_csv('./drive/My Drive/TFM/dforder.csv', encoding = 'latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcew7ABFj7kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd543be7-537d-4766-e6e3-c82a3c83dbb4"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                     tweet                   id\n",
            "0        event great place set sustainability trend par...                   12\n",
            "1            hahaha patigue plus climate change ganun lang                   12\n",
            "2        recommend people use paper reuse times polyest...                   12\n",
            "3                                        wow strong speech                   13\n",
            "4                                go tag lovely finite look                   17\n",
            "...                                                    ...                  ...\n",
            "5509133                          shit honestly scary think  1130011369850281984\n",
            "5509134  way go productive conversation prepare natural...  1130075956989878272\n",
            "5509135               good luck ireland good time bad time  1130109507915399169\n",
            "5509136  social medium great platform increase awarenes...  1130120404369072128\n",
            "5509137  walk tall thing try stop think climate change ...  1130140535455387653\n",
            "\n",
            "[5509138 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymxqYAvwaN7"
      },
      "source": [
        "#### Process of lemmatizing the tweets of the dataset####################\r\n",
        "def lemmatization(text):\r\n",
        "    '''Lemmatize the processed tweets \r\n",
        "    The tweets are previously cleaned \r\n",
        "    of stopwords, punctuation, urls, etc.'''\r\n",
        "    lemmas = []\r\n",
        "    \r\n",
        "    doc = nlp(text)\r\n",
        "    \r\n",
        "    # Something goes here :P\r\n",
        "    for token in doc: \r\n",
        "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\r\n",
        "            lemmas.append(token.lemma_)\r\n",
        "    \r\n",
        "    return lemmas\r\n",
        "\r\n",
        "L = df.apply(lemmatization)\r\n",
        "# Make lemmas a string again\r\n",
        "LT = [' '.join(map(str, l)) for l in L]\r\n",
        "LT = pd.DataFrame(LT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6JbGY3l7YVG",
        "outputId": "0f2f9c6d-0259-4be5-9a3b-6f9f51153869"
      },
      "source": [
        "# This function is to tokenize the dataset ##################################\r\n",
        "def tokenize(text):\r\n",
        "    \"\"\"\r\n",
        "    Parses a string into a list of semantic units (words)\r\n",
        "    Args:\r\n",
        "        text (str): The string that the function will tokenize.\r\n",
        "    Returns:\r\n",
        "        list: tokens parsed out\r\n",
        "    \"\"\"\r\n",
        "    text = str(text)\r\n",
        "    tokens = text.split() # Make text lowercase and split it\r\n",
        "    \r\n",
        "    return tokens\r\n",
        "\r\n",
        "# Apply tokenizer\r\n",
        "df['tokens'] = df['tweet'].apply(tokenize)\r\n",
        "print(df['tokens'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0          [event, great, place, set, sustainability, tre...\n",
            "1          [hahaha, patigue, plus, climate, change, ganun...\n",
            "2          [recommend, people, use, paper, reuse, times, ...\n",
            "3                                      [wow, strong, speech]\n",
            "4                            [go, tag, lovely, finite, look]\n",
            "                                 ...                        \n",
            "5509133                       [shit, honestly, scary, think]\n",
            "5509134    [way, go, productive, conversation, prepare, n...\n",
            "5509135         [good, luck, ireland, good, time, bad, time]\n",
            "5509136    [social, medium, great, platform, increase, aw...\n",
            "5509137    [walk, tall, thing, try, stop, think, climate,...\n",
            "Name: tokens, Length: 5509138, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I52zx6Mv0JCv"
      },
      "source": [
        "#REMOVE any tweet that has NAN\r\n",
        "dft= df['x']\r\n",
        "dft = [x for x in dft if str(x) != 'nan']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZnq6xMw4V09"
      },
      "source": [
        "### THIS IS THE FUNCTION FOR TF-IDF of UNIGRAM#####################################\r\n",
        "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\r\n",
        "tfIdf = tfIdfVectorizer.fit_transform(dft)\r\n",
        "names=tfIdfVectorizer.get_feature_names()\r\n",
        "freqs = tfIdf.sum(axis=0).A1\r\n",
        "result = dict(zip(names, freqs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHieIGpO4dmt"
      },
      "source": [
        "# We will then obtain the first 30 most popular words in the TF-IDF####\r\n",
        "from operator import itemgetter\r\n",
        "i = 0\r\n",
        "for key, value in sorted(result.items(), key = itemgetter(1), reverse = True):\r\n",
        "    i += 1\r\n",
        "    if i == 30:\r\n",
        "      break\r\n",
        "    print(key, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4DUXAa94hZd"
      },
      "source": [
        "### THIS IS THE FUNCTION FOR TF-IDF of BIGRAM#####################################\r\n",
        "tfIdfVectorizer2=TfidfVectorizer(use_idf=True, ngram_range=(2,2))\r\n",
        "tfIdf2 = tfIdfVectorizer2.fit_transform(dft)\r\n",
        "names2=tfIdfVectorizer2.get_feature_names()\r\n",
        "freqs2 = tfIdf2.sum(axis=0).A1\r\n",
        "result2 = dict(zip(names2, freqs2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZG0dkME4kUE"
      },
      "source": [
        "# Obtain the first 30 most popular words in the TF-IDF####\r\n",
        "from operator import itemgetter\r\n",
        "i = 0\r\n",
        "for key, value in sorted(result2.items(), key = itemgetter(1), reverse = True):\r\n",
        "    i += 1\r\n",
        "    if i == 30:\r\n",
        "      break\r\n",
        "    print(key, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GNV_9Kz4rce"
      },
      "source": [
        "### THIS IS THE FUNCTION FOR TF-IDF of TRIGRAM#####################################\r\n",
        "tfIdfVectorizer3=TfidfVectorizer(use_idf=True, ngram_range=(3,3))\r\n",
        "tfIdf3 = tfIdfVectorizer3.fit_transform(dft)\r\n",
        "names3= tfIdfVectorizer3.get_feature_names()\r\n",
        "freqs3 = tfIdf3.sum(axis=0).A1\r\n",
        "result3 = dict(zip(names3, freqs3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHFupXph4n7G"
      },
      "source": [
        "# Obtain the first 30 most popular words in the TF-IDF####\r\n",
        "from operator import itemgetter\r\n",
        "i = 0\r\n",
        "for key, value in sorted(result3.items(), key = itemgetter(1), reverse = True):\r\n",
        "    i += 1\r\n",
        "    if i == 30:\r\n",
        "      break\r\n",
        "    print(key, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzb2__TI-7F-"
      },
      "source": [
        "####MODEL####################################################################################################\r\n",
        "#Download author pooled data \r\n",
        "df=pd.read_csv('./drive/My Drive/TFM/df.csv', encoding = 'latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yotce3gc_2mg"
      },
      "source": [
        "# Tokenize the data\r\n",
        "def tokenize(text):\r\n",
        "    \"\"\"\r\n",
        "    Parses a string into a list of semantic units (words)\r\n",
        "    Args:\r\n",
        "        text (str): The string that the function will tokenize.\r\n",
        "    Returns:\r\n",
        "        list: tokens parsed out\r\n",
        "    \"\"\"\r\n",
        "    text = str(text)\r\n",
        "    tokens = text.split() # Make text lowercase and split it\r\n",
        "    \r\n",
        "    return tokens\r\n",
        "\r\n",
        "# Apply tokenizer\r\n",
        "#df['tokens'] = df['tweet'].apply(tokenize)\r\n",
        "df['tokens']= df['x'].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B4KmZSb7bZ3",
        "outputId": "5dc54075-5cf4-4dd5-a881-2b933797308c"
      },
      "source": [
        "#Importing seed for reproducibility\r\n",
        "from random import seed\r\n",
        "seed(24)\r\n",
        "# Create a dictionary of words for topic modeling\r\n",
        "id2word = Dictionary(df['tokens']) #449930"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "449929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L7LB5jD7dLW",
        "outputId": "3735b711-ae00-4a4a-9fa9-d6d4b6ba0f05"
      },
      "source": [
        "# Filtering Extremes\r\n",
        "id2word.filter_extremes(no_below=2, no_above=.95)#100000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpZQuzZq7fCu",
        "outputId": "b52b9a28-076e-4c94-a449-77485f226fbe"
      },
      "source": [
        "# Creating a corpus object \r\n",
        "corpus = [id2word.doc2bow(d) for d in df['tokens']]\r\n",
        "print(corpus[:1][0][:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KilbBoiN9rWb"
      },
      "source": [
        "# BASE MODEL __________________________________________________________________\r\n",
        "#number of topics k \r\n",
        "k=10\r\n",
        "# LDA model \r\n",
        "base_model = gensim.models.LdaMulticore(corpus=corpus, \r\n",
        "                                        id2word=id2word, \r\n",
        "                                        workers=3,\r\n",
        "                                        num_topics=k,\r\n",
        "                                        passes=5,\r\n",
        "                                        random_state= 24)\r\n",
        "# Compute Coherence Score \r\n",
        "coherence_model2 = CoherenceModel(model=base_model, texts=df['tokens'], \r\n",
        "                                   dictionary=id2word, coherence='c_v')\r\n",
        "coherence2 = coherence_model2.get_coherence()\r\n",
        "print('\\nCoherence Score: ', coherence2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBsmyS8n__df"
      },
      "source": [
        "# Print the Keyword in the 10 topics\r\n",
        "print(base_model.print_topics())\r\n",
        "doc_lda = base_model[corpus]\r\n",
        "# Filtering for words \r\n",
        "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\r\n",
        "# Create Topics\r\n",
        "topics = [' '.join(t[0:10]) for t in words]\r\n",
        "# Getting the topics\r\n",
        "for id, t in enumerate(topics): \r\n",
        "    print(f\"------ Topic {id} ------\")\r\n",
        "    print(t, end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0F8V0ZmADRX"
      },
      "source": [
        "#Creating Topic Distance Visualization \r\n",
        "pyLDAvis.enable_notebook()\r\n",
        "pyLDAvis.gensim.prepare(base_model, corpus, id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV3YOdIdbfhT"
      },
      "source": [
        "# Finding optimal k ############################################################\r\n",
        "#PLEASE TAKE INTO COSNDIERATION THAT THIS MDOEL TAKES A LONG TIME ##############\r\n",
        "#You may break it into parts \r\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):\r\n",
        "    \"\"\"\r\n",
        "    Compute c_v coherence for various number of topics\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    ----------\r\n",
        "    dictionary : Gensim dictionary\r\n",
        "    corpus : Gensim corpus\r\n",
        "    texts : List of input texts\r\n",
        "    limit : Max num of topics\r\n",
        "    Returns:\r\n",
        "    -------\r\n",
        "    model_list : List of LDA topic models\r\n",
        "    coherence_values : Coherence values corresponding to the \r\n",
        "    LDA model with respective number of topics\r\n",
        "    \"\"\"\r\n",
        "    coherence_values_topic = []\r\n",
        "    model_list_topic = []\r\n",
        "    for num_topics in range(start, limit, step):\r\n",
        "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word)\r\n",
        "        model_list_topic.append(model)\r\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\r\n",
        "        coherence_values_topic.append(coherencemodel.get_coherence())\r\n",
        "\r\n",
        "    return model_list_topic, coherence_values_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp6Rksl1lFSf"
      },
      "source": [
        "# This is to apply the above function which may take a long time \r\n",
        "model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,\r\n",
        "                                                        corpus=corpus,\r\n",
        "                                                        texts=df['tokens'],\r\n",
        "                                                        start=2, limit=50, step=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCGwsAO_xMbc"
      },
      "source": [
        "#COMPARING K for 17, 25, and 40 ################################################\r\n",
        "# LDA model \r\n",
        "model17 = gensim.models.LdaMulticore(corpus=corpus, \r\n",
        "                                        id2word=id2word, \r\n",
        "                                        workers=3,\r\n",
        "                                        num_topics=17,\r\n",
        "                                        passes=5,\r\n",
        "                                        random_state= 24)\r\n",
        "model25 = gensim.models.LdaMulticore(corpus=corpus, \r\n",
        "                                        id2word=id2word, \r\n",
        "                                        workers=3,\r\n",
        "                                        num_topics=25,\r\n",
        "                                        passes=5,\r\n",
        "                                        random_state= 24)\r\n",
        "model40 = gensim.models.LdaMulticore(corpus=corpus, \r\n",
        "                                        id2word=id2word, \r\n",
        "                                        workers=3,\r\n",
        "                                        num_topics=40,\r\n",
        "                                        passes=5,\r\n",
        "                                        random_state= 24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBQ1mcL5xk_1"
      },
      "source": [
        "#Creating Topic Distance Visualization \r\n",
        "pyLDAvis.enable_notebook()\r\n",
        "pyLDAvis.gensim.prepare(model17, corpus, id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tJ4tTpJxnfY"
      },
      "source": [
        "#Creating Topic Distance Visualization \r\n",
        "pyLDAvis.enable_notebook()\r\n",
        "pyLDAvis.gensim.prepare(model25, corpus, id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_RxbwE6xnxC"
      },
      "source": [
        "#Creating Topic Distance Visualization \r\n",
        "pyLDAvis.enable_notebook()\r\n",
        "pyLDAvis.gensim.prepare(model40, corpus, id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vnJZs0cBJCB"
      },
      "source": [
        "# FINDING OPTIMAL ALPHA AND ETA ################################################\r\n",
        "#this takes a long time, you may break it into parts \r\n",
        "#This is the function to obtain the best alpha and eta according to coherence score\r\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\r\n",
        "    \r\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\r\n",
        "                                           id2word=dictionary,\r\n",
        "                                           num_topics=k, \r\n",
        "                                           random_state=100,\r\n",
        "                                           chunksize=100,\r\n",
        "                                           passes=10,\r\n",
        "                                           alpha=a,\r\n",
        "                                           eta=b)\r\n",
        "    \r\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\r\n",
        "    \r\n",
        "    return coherence_model_lda.get_coherence()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMUG3_hTARje"
      },
      "source": [
        "import numpy as np\r\n",
        "import tqdm\r\n",
        "grid = {}\r\n",
        "grid['Validation_Set'] = {}\r\n",
        "# Topics range\r\n",
        "k = 17\r\n",
        "# Alpha parameter\r\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\r\n",
        "alpha.append('symmetric')\r\n",
        "alpha.append('asymmetric')\r\n",
        "# eta parameter\r\n",
        "eta = list(np.arange(0.01, 1, 0.3))\r\n",
        "eta.append('symmetric')\r\n",
        "# Validation sets\r\n",
        "num_of_docs = len(corpus)\r\n",
        "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \r\n",
        "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \r\n",
        "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \r\n",
        "               corpus]\r\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\r\n",
        "model_results = {'Validation_Set': [],\r\n",
        "                 'Alpha': [],\r\n",
        "                 'Eta': [],\r\n",
        "                 'Coherence': []\r\n",
        "                }\r\n",
        "# Can take a long time to run\r\n",
        "if 1 == 1:\r\n",
        "    pbar = tqdm.tqdm(total=540)\r\n",
        "    \r\n",
        "    # iterate through validation corpuses\r\n",
        "    for i in range(len(corpus_sets)):\r\n",
        "        # iterate through alpha values\r\n",
        "        for a in alpha:\r\n",
        "            # iterare through eta values\r\n",
        "            for b in eta:\r\n",
        "                # get the coherence score for the given parameters\r\n",
        "                cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \r\n",
        "                                              k=17, a=a, b=b)\r\n",
        "                # Save the model results\r\n",
        "                model_results['Validation_Set'].append(corpus_title[i])\r\n",
        "                model_results['Alpha'].append(a)\r\n",
        "                model_results['Eta'].append(b)\r\n",
        "                model_results['Coherence'].append(cv)\r\n",
        "                    \r\n",
        "                pbar.update(1)\r\n",
        "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\r\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZgeXe6EiEd1"
      },
      "source": [
        "#ITERATIONS ####################################################################\r\n",
        "# Finding optimal k ############################################################\r\n",
        "#PLEASE TAKE INTO COSNDIERATION THAT THIS MDOEL TAKES A LONG TIME ##############\r\n",
        "#You may break it into parts \r\n",
        "def compute_coherence_values(dictionary, corpus, num_topics= 17, texts, limit, start=50, step=10):\r\n",
        "    \"\"\"\r\n",
        "    Compute c_v coherence for various number of topics\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    ----------\r\n",
        "    dictionary : Gensim dictionary\r\n",
        "    corpus : Gensim corpus\r\n",
        "    texts : List of input texts\r\n",
        "    limit : Max num of iterations\r\n",
        "    Returns:\r\n",
        "    -------\r\n",
        "    model_list : List of LDA topic models\r\n",
        "    coherence_values : Coherence values corresponding to the \r\n",
        "    LDA model with respective number of topics\r\n",
        "    \"\"\"\r\n",
        "    coherence_values_topic = []\r\n",
        "    model_list_topic = []\r\n",
        "    for it in range(start, limit, step):\r\n",
        "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word, iterations = it, workers=3)\r\n",
        "        model_list_topic.append(model)\r\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\r\n",
        "        coherence_values_topic.append(coherencemodel.get_coherence())\r\n",
        "\r\n",
        "    return model_list_topic, coherence_values_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0AK14ceUyav"
      },
      "source": [
        "# This is to apply the above function which may take a long time \r\n",
        "model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,\r\n",
        "                                                        corpus=corpus,\r\n",
        "                                                        texts=df['tokens'],\r\n",
        "                                                        start=50, limit=150, step=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHihHrqmR-tv"
      },
      "source": [
        "#SENTIMENT ANALYSIS____________________________________________________________\r\n",
        "df=pd.read_csv('./drive/My Drive/TFM/tte.csv', encoding = 'latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYy_WfhHSHT6"
      },
      "source": [
        "# Tokenizer function\r\n",
        "def tokenize(text):\r\n",
        "    \"\"\"\r\n",
        "    Parses a string into a list of semantic units (words)\r\n",
        "    Args:\r\n",
        "        text (str): The string that the function will tokenize.\r\n",
        "    Returns:\r\n",
        "        list: tokens parsed out\r\n",
        "    \"\"\"\r\n",
        "    # Removing url's\r\n",
        "    text = re.sub(r'http\\S+', '', text)# https://www.youtube.com/watch?v=O2onA4r5UaY\r\n",
        "    text = re.sub('@*#*', '', text) # Remove @ $ # \r\n",
        "   # text = remove_users(text)\r\n",
        "   # text = remove_links(text)\r\n",
        "    return text\r\n",
        "\r\n",
        "# Apply tokenizer\r\n",
        "SA = df['x'].apply(tokenize)\r\n",
        "SA = pd.DataFrame(SA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXDuYSHBSdO-",
        "outputId": "4b70c6e4-ff35-4547-bda4-42a07877f6b1"
      },
      "source": [
        "print(SA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                          x\n",
            "0         An eye-opening article. This further reinforce...\n",
            "1         Bangladesh Confronts Climate Change - book goe...\n",
            "2         If thereâs a definition of insanity itâs A...\n",
            "3         The scene in Jurassic Park when Newman loses h...\n",
            "4         The scene in Jurassic Park when Newman loses h...\n",
            "...                                                     ...\n",
            "26778407  DYK climatechange is impacting snow leopards? ...\n",
            "26778408  Once spoke to a woman you said she's hoping th...\n",
            "26778409  Poor country scientists to get help to study g...\n",
            "26778410  Take our climate pledge and do your part to re...\n",
            "26778411  Earth to Trump: you are too late, science of c...\n",
            "\n",
            "[26778412 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3993F6fMRaJx"
      },
      "source": [
        "# function of tweet scores \r\n",
        "def tweet_scores(data_frame): \r\n",
        "  \r\n",
        "    # Create a SentimentIntensityAnalyzer object. \r\n",
        "    sid_obj = SentimentIntensityAnalyzer()\r\n",
        "    scores = []\r\n",
        "    n = data_frame.shape[0]\r\n",
        "    for i in range(n):\r\n",
        "      sentence = data_frame.iloc[i]\r\n",
        "      sentiment_dict = sid_obj.polarity_scores(sentence)\r\n",
        "      compound = sentiment_dict['compound']\r\n",
        "      scores.append(compound)\r\n",
        "    return scores\r\n",
        "twtscr= tweet_scores(SA)\r\n",
        "twtsa = pd.DataFrame(twtscr)\r\n",
        "twtsa.columns = ['comp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wff162JaIv4-",
        "outputId": "c9d56ded-9d54-4532-8a3d-d2f213db3c65"
      },
      "source": [
        "print(twtsa.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     comp\n",
            "0  0.8430\n",
            "1 -0.2263\n",
            "2 -0.9136\n",
            "3 -0.3182\n",
            "4 -0.3182\n",
            "5  0.1531\n",
            "6  0.0000\n",
            "7  0.7777\n",
            "8 -0.7906\n",
            "9 -0.7906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLPzuivpRikg"
      },
      "source": [
        "twtsa.to_csv('twtsa.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "KsE4-PhaHjI8",
        "outputId": "f07d7af6-161b-45a8-b1e1-83481548e5ed"
      },
      "source": [
        "twtsa.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.677841e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-5.732172e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.921727e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-9.988000e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-3.826000e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.818000e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.997000e-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               comp\n",
              "count  2.677841e+07\n",
              "mean  -5.732172e-03\n",
              "std    4.921727e-01\n",
              "min   -9.988000e-01\n",
              "25%   -3.826000e-01\n",
              "50%    0.000000e+00\n",
              "75%    3.818000e-01\n",
              "max    9.997000e-01"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "KYFiBSu8IpNM",
        "outputId": "5bc6e32e-9ce2-4f77-91a9-67686988c892"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "twtsa.boxplot(column='comp', figsize=(12,8), showmeans=True)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7a9903bb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW6A2jKsP71r"
      },
      "source": [
        "# SENSITIVITY ANALYSIS OF THE SENTIMENT ANALYSIS################################\r\n",
        "threshold = np.linspace(0.1,0.01,30)\r\n",
        "total = []\r\n",
        "for j in threshold:\r\n",
        "  results= []\r\n",
        "  for i in range(len(twtsa)):\r\n",
        "    if twtsa.iloc[i,0] >= j: \r\n",
        "      result = 1\r\n",
        "    elif twtsa.iloc[i,0] <= -j:\r\n",
        "      result= -1\r\n",
        "    else:\r\n",
        "      result= 0\r\n",
        "    results.append(result)\r\n",
        "    \r\n",
        "  neutral = results.count(0)\r\n",
        "  positive = results.count(1)\r\n",
        "  negative = results.count(-1)\r\n",
        "  total.append([positive,neutral,negative])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcWbz5u4lnUz"
      },
      "source": [
        "n= 26778412\r\n",
        "percentages= satotal/n\r\n",
        "print(percentages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtpelqCuewV1"
      },
      "source": [
        "fig = plt.figure(figsize =(8,5)) \r\n",
        "# Create dictionary of keyword aruments to pass to plt.boxplot\r\n",
        "dict2 =  {'patch_artist': True,\r\n",
        "             'capprops': dict(color='black'), 'boxprops': dict(color='lightblue', facecolor='lightblue'),\r\n",
        "             'medianprops': dict(color='black'),\r\n",
        "             'whiskerprops': dict(color='black')}\r\n",
        "# Creating plot \r\n",
        "bplot = percentages.boxplot(column= ['Positive','Neutral', 'Negative'],**dict2) #, 'Neutral', 'Negative'\r\n",
        "plt.ylabel('Percentage of Tweets')\r\n",
        "plt.xlabel('Type of Sentiment')\r\n",
        "#plt.ylim((0,0.42))\r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a24wo6yAyvaT"
      },
      "source": [
        "### Donwload results ########################3\r\n",
        "df=pd.read_csv('./drive/My Drive/TFM/results.csv', encoding = 'latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hOvJzFCcbw7",
        "outputId": "6b3dcea6-ffda-403e-b0f7-45c268c9c603"
      },
      "source": [
        "#df.columns = ['num','comp']\r\n",
        "df= df['comp']\r\n",
        "print(df.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1   -1\n",
            "2   -1\n",
            "3   -1\n",
            "4   -1\n",
            "5    1\n",
            "6    0\n",
            "7    1\n",
            "8   -1\n",
            "9   -1\n",
            "Name: comp, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY8JDYWBaw-i",
        "outputId": "af46d363-9774-4577-83e2-71349e10b421"
      },
      "source": [
        "(df == 0).sum() # 5228965\r\n",
        "(df == 1).sum() #10886190\r\n",
        "(df == -1).sum() #10663257"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10886190"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHzawv4G-aOw"
      },
      "source": [
        "#Dowload compound scores \r\n",
        "twtsa=pd.read_csv('./drive/My Drive/TFM/twtsa.csv', encoding = 'latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUsYJfkBHaJ4",
        "outputId": "ff005248-a87c-4abe-95e2-d24ce6c19452"
      },
      "source": [
        "print(twtsa.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0    comp\n",
            "0           0  0.8430\n",
            "1           1 -0.2263\n",
            "2           2 -0.9136\n",
            "3           3 -0.3182\n",
            "4           4 -0.3182\n",
            "5           5  0.1531\n",
            "6           6  0.0000\n",
            "7           7  0.7777\n",
            "8           8 -0.7906\n",
            "9           9 -0.7906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIK7qWZYEqG6",
        "outputId": "aa34b0e2-af7e-447d-9fd8-14cc8b3ef521"
      },
      "source": [
        "results.columns = ['comp']\r\n",
        "df= results['comp']\r\n",
        "print(df.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1   -1\n",
            "2   -1\n",
            "3   -1\n",
            "4   -1\n",
            "5    1\n",
            "6    0\n",
            "7    1\n",
            "8   -1\n",
            "9   -1\n",
            "Name: comp, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj30zGZiyY5H",
        "outputId": "b2c1d0be-6d66-4f32-8c4b-94a67dfb84ce"
      },
      "source": [
        "#Check how many cpu's you have in your computer \r\n",
        "import multiprocessing\r\n",
        "\r\n",
        "multiprocessing.cpu_count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO_ScZIyn6rQ"
      },
      "source": [
        "#  TEMPORALITY OF SENTIMENT ANALYSIS ###########################################\r\n",
        "#joining the date and the sentiment analysis\r\n",
        "#date= pd.date_range(start=\"2017-09-21\",end=\"2019-05-19\").tolist()\r\n",
        "#date= pd.DataFrame(date)\r\n",
        "#date.columns= ['date']\r\n",
        "#temp['date'] = temp['date'].astype('datetime64[ns]')\r\n",
        "df2= pd.concat([df.reset_index(drop=True), date.reset_index(drop=True)], axis=1)\r\n",
        "#sort by date\r\n",
        "df = df2.sort_values(by='date')\r\n",
        "#summing by date the sentiment analysis \r\n",
        "temp= df.groupby(['date'])['sa'].sum()\r\n",
        "temp.reset_index(drop=True, inplace=True)\r\n",
        "import datetime\r\n",
        "temp['date']= [datetime.datetime.strptime(d,\"%Y-%m-%d\").date() for d in temp['date']]\r\n",
        "#date.columns= ['date']\r\n",
        "#temp['date'] = temp['date'].astype('datetime64[ns]')\r\n",
        "plotsa= pd.merge(date, temp, on='date', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6mtjUVmtJLK"
      },
      "source": [
        "sortsa= plotsa.sort_values(by=['sa'],ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatDNkUjtTZu"
      },
      "source": [
        "plt.plot(plotsa['date'],plotsa['sa'])\r\n",
        "plt.xlabel('Date')\r\n",
        "plt.ylabel('Sentiment Score')\r\n",
        "plt.tick_params(axis='x', rotation=90)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YkDloXktbYx"
      },
      "source": [
        "#confidence intervals of sentiment analysis\r\n",
        "import scipy.stats as st\r\n",
        "ci_pos= st.t.interval(alpha=0.95, df=len(satotal['Positive'])-1, loc=np.mean(satotal['Positive']), scale=st.sem(satotal['Positive'])) #(10721008.287961453, 10879864.84537188)\r\n",
        "ci_neu= st.t.interval(alpha=0.95, df=len(satotal['Neutral'])-1, loc=np.mean(satotal['Neutral']), scale=st.sem(satotal['Neutral'])) #(5252915.313658239, 5553480.8196750935)\r\n",
        "ci_neg= st.t.interval(alpha=0.95, df=len(satotal['Negative'])-1, loc=np.mean(satotal['Negative']), scale=st.sem(satotal['Negative'])) #(10503622.979412757, 10645931.753920577)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzQlVhJcuA5n"
      },
      "source": [
        "n= 26778412\r\n",
        "percentages= satotal/n\r\n",
        "print(percentages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnilH8mfuDMh"
      },
      "source": [
        "# visualization of type of sentiment on all dataset \r\n",
        "fig = plt.figure(figsize =(8,5)) \r\n",
        "# Create dictionary of keyword aruments to pass to plt.boxplot\r\n",
        "dict2 =  {'patch_artist': True,\r\n",
        "             'capprops': dict(color='black'), 'boxprops': dict(color='lightblue', facecolor='lightblue'),\r\n",
        "             'medianprops': dict(color='black'),\r\n",
        "             'whiskerprops': dict(color='black')}\r\n",
        "# Creating plot \r\n",
        "bplot = percentages.boxplot(column= ['Neutral'],**dict2) #, 'Neutral', 'Negative'\r\n",
        "plt.ylabel('Percentage of Tweets')\r\n",
        "plt.xlabel('Type of Sentiment')\r\n",
        "#plt.ylim((0,0.42))\r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3jEeGvKnAqv"
      },
      "source": [
        "########################Distribution of topcis over the document################\r\n",
        "dff=pd.read_csv('./drive/My Drive/TFM/dforder.csv', encoding = 'latin1') #download dataset of all tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzb5LVhrnb5T"
      },
      "source": [
        "# Tokenize datset of all tweets\r\n",
        "def tokenize(text):\r\n",
        "    \"\"\"\r\n",
        "    Parses a string into a list of semantic units (words)\r\n",
        "    Args:\r\n",
        "        text (str): The string that the function will tokenize.\r\n",
        "    Returns:\r\n",
        "        list: tokens parsed out\r\n",
        "    \"\"\"\r\n",
        "    text = str(text)\r\n",
        "    tokens = text.split() # Make text lowercase and split it\r\n",
        "    \r\n",
        "    return tokens\r\n",
        "\r\n",
        "# Apply tokenizer\r\n",
        "dff2['tokens'] = dff2['tweet'].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RPW9mqBnzqE"
      },
      "source": [
        "#Donwnload model\r\n",
        "model17_3 =  models.LdaModel.load('model17_3.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6FSWxn8nlUc"
      },
      "source": [
        "# Create a dictionary\r\n",
        "id2word = Dictionary(df['tokens'])\r\n",
        "print(len(id2word)) #449930\r\n",
        "# Filtering Extremes\r\n",
        "id2word.filter_extremes(no_below=2, no_above=.95)\r\n",
        "print(len(id2word))#100000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYpiBBkynWat"
      },
      "source": [
        "# Creating a corpus object of the dataset with all tweets\r\n",
        "corpus = [id2word.doc2bow(d) for d in dff2['tokens']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoJqoPLwnSbc"
      },
      "source": [
        "#FUNCTION to link each tweet with its highest probable topic \r\n",
        "def format_topics_sentences(ldamodel=model17_3, corpus=corpus, texts=dff2['tweet']):\r\n",
        "    # Init output\r\n",
        "    sent_topics_df = pd.DataFrame()\r\n",
        "\r\n",
        "    # Get main topic in each document\r\n",
        "    for i, row in enumerate(ldamodel[corpus]):\r\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\r\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\r\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\r\n",
        "            if j == 0:  # => dominant topic\r\n",
        "                wp = ldamodel.show_topic(topic_num)\r\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\r\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\r\n",
        "            else:\r\n",
        "                break\r\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\r\n",
        "\r\n",
        "    # Add original text to the end of the output\r\n",
        "    contents = pd.Series(texts)\r\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\r\n",
        "    return(sent_topics_df)\r\n",
        "\r\n",
        "\r\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=model17_3, corpus=corpus, texts=dff2['tweet'])\r\n",
        "\r\n",
        "# Format\r\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\r\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\r\n",
        "\r\n",
        "# Show\r\n",
        "df_dominant_topic.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Osv_MJYn4fv"
      },
      "source": [
        "# Number of Documents for Each Topic\r\n",
        "topic_counts = df_dominant_topic['Dominant_Topic'].value_counts()\r\n",
        "# Percentage of Documents for Each Topic\r\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\r\n",
        "# Topic Number and Keywords\r\n",
        "topic_num_keywords = df_dominant_topic[['Dominant_Topic', 'Keywords']]\r\n",
        "\r\n",
        "# Concatenate Column wise and change column name\r\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\r\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Keywords', 'Num_Documents', 'Perc_Documents']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc6J07rIbBUA"
      },
      "source": [
        "topic_key= pd.DataFrame(topic_key)\r\n",
        "topic_key.columns=['Dominant_Topic', 'Topic_Keywords']\r\n",
        "topic_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1KVRrAGbKSH"
      },
      "source": [
        "# Number of Documents for Each Topic WITH CLEANED DATA########################################\r\n",
        "topic_counts = dt['Dominant_Topic'].value_counts()\r\n",
        "\r\n",
        "# Percentage of Documents for Each Topic\r\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\r\n",
        "\r\n",
        "# Topic Number and Keywords\r\n",
        "topic_num_keywords = topic_key[['Dominant_Topic', 'Topic_Keywords']]\r\n",
        "\r\n",
        "# Concatenate Column wise & Change Column names\r\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\r\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\r\n",
        "\r\n",
        "df_dominant_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd7jVROPbCEZ"
      },
      "source": [
        "#Find the most representative document for each topic##########################\r\n",
        "# Group top 5 sentences under each topic\r\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\r\n",
        "\r\n",
        "sent_topics_outdf_grpd = dt.groupby('Dominant_Topic')\r\n",
        "\r\n",
        "for i, grp in sent_topics_outdf_grpd:\r\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \r\n",
        "                                             grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(1)], \r\n",
        "                                            axis=0)\r\n",
        "\r\n",
        "# Reset Index    \r\n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6wZ_yHCI6Oc"
      },
      "source": [
        "#TEMPORALITY OF SA##############################################################\r\n",
        "#date= pd.date_range(start=\"2017-09-21\",end=\"2019-05-19\").tolist()\r\n",
        "#date= pd.DataFrame(date)\r\n",
        "#date.columns= ['date']\r\n",
        "#temp['date'] = temp['date'].astype('datetime64[ns]')\r\n",
        "df2= pd.concat([df.reset_index(drop=True), date.reset_index(drop=True)], axis=1)\r\n",
        "#sort by date\r\n",
        "df = df2.sort_values(by='date')\r\n",
        "###\r\n",
        "temp= df.groupby(['date'])['sa'].sum()\r\n",
        "plotsa= pd.merge(date, temp, on='date', how='left')\r\n",
        "print(plotsa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1dzouFzJ5Bn"
      },
      "source": [
        "sortsa= plotsa.sort_values(by=['sa'],ascending=False)\r\n",
        "print(sortsa[0:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP2zDIkzb7ya"
      },
      "source": [
        "plt.plot(plotsa['date'],plotsa['sa'])\r\n",
        "plt.xlabel('Date')\r\n",
        "plt.ylabel('Sentiment Score')\r\n",
        "plt.tick_params(axis='x', rotation=90)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8MAImEZc51o"
      },
      "source": [
        "#Download data for sentiment analysis\r\n",
        "dt=pd.read_csv('./drive/My Drive/TFM/df_dominant_topic.csv', encoding = 'latin1')\r\n",
        "date=pd.read_csv('./drive/My Drive/TFM/date2.csv', encoding = 'latin1')\r\n",
        "results=pd.read_csv('./drive/My Drive/TFM/results.csv', encoding = 'latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc1t4YR-bTLY"
      },
      "source": [
        "#############################################\r\n",
        "#merging topics with sentiment analisis \r\n",
        "result= pd.concat([dt, date, results], axis=1))\r\n",
        "result.dropna(subset = [\"Text\"], inplace=True)\r\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3m0vghncTHi"
      },
      "source": [
        "# frequency by TOPIC ######################################################################\r\n",
        "#Do this for every topic \r\n",
        "# 0,8,1, 11,3 \r\n",
        "topic1= result[result.Dominant_Topic== 0]\r\n",
        "topic1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5caVzkWd39d"
      },
      "source": [
        "temp1=pd.crosstab(index=topic1['date'], columns='count')\r\n",
        "temp1.reset_index(level=0, inplace=True)\r\n",
        "temp1.columns= ['date', 'count']\r\n",
        "temp1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCusrh6Td9ud"
      },
      "source": [
        "dd= pd.date_range(start=\"2017-10-23\",end=\"2019-05-19\").tolist()\r\n",
        "dd= pd.DataFrame(dd)\r\n",
        "dd.columns= ['date']\r\n",
        "temp1['date'] = temp1['date'].astype('datetime64[ns]')\r\n",
        "plott1= pd.merge(dd, temp1, on='date', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0wRtGsaeFD8"
      },
      "source": [
        "fig = plt.figure(figsize=(14,5))\r\n",
        "axis = brokenaxes(ylims=((0, 70000),(120000, 140000)), hspace=0.2)\r\n",
        "axis.plot(plott1['date'],plott1['count'],color = 'darkgreen')\r\n",
        "axis.grid(axis='both', which='major', ls='--')\r\n",
        "plt.ylabel('Daily Sentiment Analysis of the UK',labelpad=70)\r\n",
        "plt.xlabel('Date',labelpad=40)\r\n",
        "plt.rc('font', size=16) \r\n",
        "plt.plot()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9es9DgGfU_c"
      },
      "source": [
        "############# SA BY COUNTRY#############################################################3\r\n",
        "#Download users location preprocessed\r\n",
        "final=pd.read_csv('./drive/My Drive/TFM/final_loc.csv', encoding = 'latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WUeA87WgZ39"
      },
      "source": [
        "find = pd.concat([final, date, results], axis=1)\r\n",
        "find #26519326final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUu6XFXrgjKk"
      },
      "source": [
        "# Clean user location\r\n",
        "find.obs[find['name']== 'usa']= 'usa'\r\n",
        "find.obs[find['name']== 'u k']= \"uk\"\r\n",
        "find.obs[find['name']== 'u k '] = \"uk\"\r\n",
        "\r\n",
        "find.obs[(find['name']== 'america') & (find['obs'].isnull())] = 'usa'\r\n",
        "\r\n",
        "usa= [\"united states of america\",\" usa\",'n y c', 'manhattan', \"brooklyn\", \"u s a\",\"new york\",\"new york city\",\"nyc\" \"florida\", \"washington\", \"california\", \"nyc\", \"portland\",\"new orleans\",\"nashville\",\"san diego\",\"los angeles\",\"denver\",\"austin\",\"seattle\",\"chicago\",\"boston\",\"san francisco\",\"texas\"]\r\n",
        "for i in usa: \r\n",
        "  find.obs[(find['name'].str.contains(i, na=False))& (find['obs'].isnull())]='usa'\r\n",
        "\r\n",
        "uk= [\" uk\",\"manchester\",\"edinburgh\",\"glasgow\",\"scotland\",\"brighton\",\"birmingham\",\"london\",\"united kingdom\",\"england\",'belfast', 'ireland', 'yorkshire', 'southampton', 'nottingham', 'leeds', 'newcastle', 'liverpool', 'dublin' ]\r\n",
        "for i in uk: \r\n",
        "  find.obs[(find['name'].str.contains(i, na=False))& (find['obs'].isnull())]='uk'\r\n",
        "\r\n",
        "cana= [\"canada\",\"canadian\",\"toronto\", 'vancouver', 'montreal', 'ottawa', 'calgary', 'quebec', 'edmonton', 'winnipeg', 'victoria', 'hamilton', 'halifax', 'british columbia', 'alberta', 'saskatchewan', 'manitoba', 'ontario', 'new brunswick', 'nova scotia', 'yukon' ]\r\n",
        "for i in cana: \r\n",
        "  find.obs[(find['name'].str.contains(i, na=False))& (find['obs'].isnull())]='canada'\r\n",
        "\r\n",
        "aus= [\"australia\",'sydney', 'melbourne', 'perth', 'adelaide','brisbane', 'gold coast', 'hobart', 'canberra', 'darwin','cairns', 'alice springs', 'wollongong', 'melbourne' ]\r\n",
        "for i in aus: \r\n",
        "  find.obs[(find['name'].str.contains(i, na=False))& (find['obs'].isnull())]='australia'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyfXn67ggkF2"
      },
      "source": [
        "# frequency by Country ######################################################################\r\n",
        "# USA, CANADA, UK, Australia\r\n",
        "country= find[find.obs== 'ca'] \r\n",
        "country"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFAaQabIg1D8"
      },
      "source": [
        "#summing by date the sentiment analysis ################################################################3\r\n",
        "temp= country.groupby(['date'])['comp'].sum()\r\n",
        "temp= pd.DataFrame(temp)\r\n",
        "temp.reset_index(level=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vG-0T6-g4WJ"
      },
      "source": [
        "#dd= pd.date_range(start=\"2017-09-21\",end=\"2019-05-19\").tolist()\r\n",
        "#dd= pd.DataFrame(dd)\r\n",
        "#dd.columns= ['date']\r\n",
        "temp['date'] = temp['date'].astype('datetime64[ns]')\r\n",
        "plotc= pd.merge(dd, temp, on='date', how='left')\r\n",
        "plotc.sort_values(by=['comp'],ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEHqKPQog92N"
      },
      "source": [
        "plt.plot(plotc['date'],plotc['comp'],color=\"darkblue\")\r\n",
        "plt.xlabel('Date')\r\n",
        "plt.ylabel('Daily Sentiment Analysis of Australia')\r\n",
        "plt.tick_params(axis='x', rotation=90)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (14,5)\r\n",
        "plt.rc('font', size=16) \r\n",
        "#plt.set_ylim([0,140000])\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}